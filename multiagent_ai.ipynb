{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVoqLbKAkyKoNduF8Xh+aX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miller00315/crew_ai_studies/blob/main/multiagent_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R8vRSTXjplu_",
        "outputId": "ae682bd7-3b95-438e-9a4a-f178276652e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting crewai\n",
            "  Downloading crewai-0.105.0-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting crewai_tools\n",
            "  Downloading crewai_tools-0.37.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting appdirs>=1.4.4 (from crewai)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting auth0-python>=4.7.1 (from crewai)\n",
            "  Downloading auth0_python-4.8.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from crewai) (1.9.0)\n",
            "Collecting chromadb>=0.5.23 (from crewai)\n",
            "  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from crewai) (8.1.8)\n",
            "Collecting instructor>=1.3.3 (from crewai)\n",
            "  Downloading instructor-1.7.3-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting json-repair>=0.25.2 (from crewai)\n",
            "  Downloading json_repair-0.39.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting json5>=0.10.0 (from crewai)\n",
            "  Downloading json5-0.10.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting jsonref>=1.1.0 (from crewai)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting litellm==1.60.2 (from crewai)\n",
            "  Downloading litellm-1.60.2-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: openai>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from crewai) (1.61.1)\n",
            "Requirement already satisfied: openpyxl>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from crewai) (3.1.5)\n",
            "Collecting opentelemetry-api>=1.22.0 (from crewai)\n",
            "  Downloading opentelemetry_api-1.30.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http>=1.22.0 (from crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.22.0 (from crewai)\n",
            "  Downloading opentelemetry_sdk-1.30.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pdfplumber>=0.11.4 (from crewai)\n",
            "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from crewai) (2.10.6)\n",
            "Collecting python-dotenv>=1.0.0 (from crewai)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting pyvis>=0.3.2 (from crewai)\n",
            "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.11/dist-packages (from crewai) (2024.11.6)\n",
            "Collecting tomli-w>=1.1.0 (from crewai)\n",
            "  Downloading tomli_w-1.2.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting tomli>=2.0.2 (from crewai)\n",
            "  Downloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting uv>=0.4.25 (from crewai)\n",
            "  Downloading uv-0.6.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from litellm==1.60.2->crewai) (3.11.13)\n",
            "Collecting httpx<0.28.0,>=0.23.0 (from litellm==1.60.2->crewai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm==1.60.2->crewai) (8.6.1)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm==1.60.2->crewai) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm==1.60.2->crewai) (4.23.0)\n",
            "Collecting tiktoken>=0.7.0 (from litellm==1.60.2->crewai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm==1.60.2->crewai) (0.21.0)\n",
            "Collecting docker>=7.1.0 (from crewai_tools)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting embedchain>=0.1.114 (from crewai_tools)\n",
            "  Downloading embedchain-0.1.127-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting lancedb>=0.5.4 (from crewai_tools)\n",
            "  Downloading lancedb-0.21.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting pyright>=1.1.350 (from crewai_tools)\n",
            "  Downloading pyright-1.1.396-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting pytube>=15.0.0 (from crewai_tools)\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from crewai_tools) (2.32.3)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.41 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.41)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.20 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.20)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.38)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.11)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.60.2->crewai) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.60.2->crewai) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.60.2->crewai) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.60.2->crewai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.60.2->crewai) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.60.2->crewai) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.60.2->crewai) (1.18.3)\n",
            "Requirement already satisfied: cryptography>=43.0.1 in /usr/local/lib/python3.11/dist-packages (from auth0-python>=4.7.1->crewai) (43.0.3)\n",
            "Requirement already satisfied: pyjwt>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from auth0-python>=4.7.1->crewai) (2.10.1)\n",
            "Requirement already satisfied: urllib3>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from auth0-python>=4.7.1->crewai) (2.3.0)\n",
            "Collecting build>=1.0.3 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading posthog-3.19.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.51b0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pypika>=0.48.9 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (1.70.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (0.15.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (3.10.15)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (13.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting alembic<2.0.0,>=1.13.1 (from embedchain>=0.1.114->crewai_tools)\n",
            "  Downloading alembic-1.15.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from embedchain>=0.1.114->crewai_tools) (4.13.3)\n",
            "Collecting chromadb>=0.5.23 (from crewai)\n",
            "  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting cohere<6.0,>=5.3 (from embedchain>=0.1.114->crewai_tools)\n",
            "  Downloading cohere-5.14.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.26.1 in /usr/local/lib/python3.11/dist-packages (from embedchain>=0.1.114->crewai_tools) (1.79.0)\n",
            "Collecting gptcache<0.2.0,>=0.1.43 (from embedchain>=0.1.114->crewai_tools)\n",
            "  Downloading gptcache-0.1.44-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting langchain-cohere<0.4.0,>=0.3.0 (from embedchain>=0.1.114->crewai_tools)\n",
            "  Downloading langchain_cohere-0.3.5-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting langchain-openai<0.3.0,>=0.2.1 (from embedchain>=0.1.114->crewai_tools)\n",
            "  Downloading langchain_openai-0.2.14-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langsmith<0.4,>=0.1.125 (from langchain_community)\n",
            "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting mem0ai<0.2.0,>=0.1.54 (from embedchain>=0.1.114->crewai_tools)\n",
            "  Downloading mem0ai-0.1.67-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting pypdf<6.0.0,>=5.0.0 (from embedchain>=0.1.114->crewai_tools)\n",
            "  Downloading pypdf-5.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting pysbd<0.4.0,>=0.3.4 (from embedchain>=0.1.114->crewai_tools)\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting schema<0.8.0,>=0.7.5 (from embedchain>=0.1.114->crewai_tools)\n",
            "  Downloading schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Collecting tiktoken>=0.7.0 (from litellm==1.60.2->crewai)\n",
            "  Downloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting tokenizers (from litellm==1.60.2->crewai)\n",
            "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.3.3->crewai) (0.16)\n",
            "Requirement already satisfied: jiter<0.9,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.3.3->crewai) (0.8.2)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.3.3->crewai) (2.27.2)\n",
            "Collecting deprecation (from lancedb>=0.5.4->crewai_tools)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: pyarrow>=14 in /usr/local/lib/python3.11/dist-packages (from lancedb>=0.5.4->crewai_tools) (18.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lancedb>=0.5.4->crewai_tools) (24.2)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.20->langchain_community) (0.3.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.41->langchain_community) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.13.3->crewai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.13.3->crewai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.13.3->crewai) (1.3.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl>=3.1.5->crewai) (2.0.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.22.0->crewai) (1.2.18)\n",
            "Collecting importlib-metadata>=6.8.0 (from litellm==1.60.2->crewai)\n",
            "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai) (1.69.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.30.0 (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.30.0 (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai)\n",
            "  Downloading opentelemetry_proto-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting protobuf<6.0,>=5.0 (from opentelemetry-proto==1.30.0->opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai)\n",
            "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting opentelemetry-semantic-conventions==0.51b0 (from opentelemetry-sdk>=1.22.0->crewai)\n",
            "  Downloading opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting pdfminer.six==20231228 (from pdfplumber>=0.11.4->crewai)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber>=0.11.4->crewai) (11.1.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber>=0.11.4->crewai)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber>=0.11.4->crewai) (3.4.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.4.2->crewai) (0.7.0)\n",
            "Collecting nodeenv>=1.6.0 (from pyright>=1.1.350->crewai_tools)\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from pyvis>=0.3.2->crewai) (7.34.0)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pyvis>=0.3.2->crewai) (4.0.2)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.11/dist-packages (from pyvis>=0.3.2->crewai) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->crewai_tools) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->crewai_tools) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Collecting Mako (from alembic<2.0.0,>=1.13.1->embedchain>=0.1.114->crewai_tools)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.2->embedchain>=0.1.114->crewai_tools) (2.6)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb>=0.5.23->crewai)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai_tools)\n",
            "  Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai_tools)\n",
            "  Downloading types_requests-2.32.0.20250306-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=43.0.1->auth0-python>=4.7.1->crewai) (1.17.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.22.0->crewai) (1.17.2)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.95.2->chromadb>=0.5.23->crewai)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (2.24.1)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (1.26.0)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (2.19.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (3.29.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (1.14.1)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (2.0.7)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from gptcache<0.2.0,>=0.1.43->embedchain>=0.1.114->crewai_tools) (5.5.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.23.0->litellm==1.60.2->crewai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.23.0->litellm==1.60.2->crewai) (0.14.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm==1.60.2->crewai) (3.21.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=5.3.0->pyvis>=0.3.2->crewai)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm==1.60.2->crewai) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.41->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.60.2->crewai) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.60.2->crewai) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.60.2->crewai) (0.23.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (2.8.2)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (3.2.2)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting langchain-experimental<0.4.0,>=0.3.0 (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai_tools)\n",
            "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pandas>=1.4.3 in /usr/local/lib/python3.11/dist-packages (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai_tools) (2.2.2)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai_tools) (0.9.0)\n",
            "Collecting psycopg2-binary<3.0.0,>=2.9.10 (from mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai_tools)\n",
            "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting pytz<2025.0,>=2024.1 (from mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai_tools)\n",
            "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting qdrant-client<2.0.0,>=1.9.1 (from mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai_tools)\n",
            "  Downloading qdrant_client-1.13.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb>=0.5.23->crewai)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.5.23->crewai) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.5.23->crewai) (1.13.1)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.23->crewai)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.51b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.23->crewai)\n",
            "  Downloading opentelemetry_instrumentation-0.51b0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting opentelemetry-util-http==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.23->crewai)\n",
            "  Downloading opentelemetry_util_http-0.51b0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.23->crewai)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb>=0.5.23->crewai)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb>=0.5.23->crewai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=0.5.23->crewai) (3.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->litellm==1.60.2->crewai) (0.28.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=0.5.23->crewai) (1.5.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai)\n",
            "  Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai) (14.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=43.0.1->auth0-python>=4.7.1->crewai) (2.22)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (1.62.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (2.4.2)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (2.7.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (0.14.1)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (1.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.60.2->crewai) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.60.2->crewai) (2024.10.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.8.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.5.23->crewai) (0.1.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.3->langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai_tools) (2025.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.2.13)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai_tools)\n",
            "  Downloading grpcio_tools-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai_tools)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.5.23->crewai)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=0.5.23->crewai) (1.3.0)\n",
            "Collecting grpcio>=1.58.0 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai_tools) (4.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai_tools) (0.6.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai_tools) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai_tools) (4.1.0)\n",
            "Downloading crewai-0.105.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.1/252.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading litellm-1.60.2-py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading crewai_tools-0.37.0-py3-none-any.whl (548 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.5/548.5 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.19-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading auth0_python-4.8.1-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading embedchain-0.1.127-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.4/211.4 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading instructor-1.7.3-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.2/79.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json_repair-0.39.1-py3-none-any.whl (20 kB)\n",
            "Downloading json5-0.10.0-py3-none-any.whl (34 kB)\n",
            "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading lancedb-0.21.0-cp39-abi3-manylinux_2_28_x86_64.whl (33.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.2/33.2 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.30.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_http-1.30.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.30.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.30.0-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading pyright-1.1.396-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli_w-1.2.0-py3-none-any.whl (6.7 kB)\n",
            "Downloading uv-0.6.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.2/16.2 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.1-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading cohere-5.14.0-py3-none-any.whl (253 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gptcache-0.1.44-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
            "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_cohere-0.3.5-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.2.14-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mem0ai-0.1.67-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Downloading onnxruntime-1.21.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_instrumentation_fastapi-0.51b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.51b0-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.51b0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_util_http-0.51b0-py3-none-any.whl (7.3 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.19.1-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.3.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
            "Downloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qdrant_client-1.13.3-py3-none-any.whl (306 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.7/306.7 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.0.20250306-py3-none-any.whl (20 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading grpcio_tools-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53769 sha256=bed5fd5a0987b586ad32913f48e456822f378e930f079d37162df1a205d5bbdc\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: schema, pytz, pypika, monotonic, durationpy, appdirs, uvloop, uvicorn, uv, types-requests, tomli-w, tomli, pytube, python-dotenv, pysbd, pyproject_hooks, pypdfium2, pypdf, psycopg2-binary, protobuf, portalocker, overrides, opentelemetry-util-http, nodeenv, mypy-extensions, mmh3, marshmallow, Mako, jsonref, json5, json-repair, jedi, importlib-metadata, humanfriendly, httpx-sse, httptools, grpcio, fastavro, deprecation, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, typing-inspect, tiktoken, starlette, pyright, posthog, opentelemetry-proto, opentelemetry-api, httpx, grpcio-tools, gptcache, docker, coloredlogs, build, alembic, tokenizers, pyvis, pydantic-settings, pdfminer.six, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, langsmith, lancedb, kubernetes, fastapi, dataclasses-json, auth0-python, qdrant-client, pdfplumber, opentelemetry-sdk, opentelemetry-instrumentation, litellm, instructor, cohere, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, mem0ai, langchain-openai, opentelemetry-instrumentation-fastapi, langchain_community, chromadb, langchain-experimental, crewai, langchain-cohere, embedchain, crewai_tools\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.1\n",
            "    Uninstalling pytz-2025.1:\n",
            "      Successfully uninstalled pytz-2025.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.6.1\n",
            "    Uninstalling importlib_metadata-8.6.1:\n",
            "      Successfully uninstalled importlib_metadata-8.6.1\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.70.0\n",
            "    Uninstalling grpcio-1.70.0:\n",
            "      Successfully uninstalled grpcio-1.70.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.16.0\n",
            "    Uninstalling opentelemetry-api-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.16.0\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.0\n",
            "    Uninstalling tokenizers-0.21.0:\n",
            "      Successfully uninstalled tokenizers-0.21.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.37b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.37b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.3.11\n",
            "    Uninstalling langsmith-0.3.11:\n",
            "      Successfully uninstalled langsmith-0.3.11\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.16.0\n",
            "    Uninstalling opentelemetry-sdk-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.48.3 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\n",
            "google-genai 1.4.0 requires httpx<1.0.0dev,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.9 alembic-1.15.1 appdirs-1.4.4 asgiref-3.8.1 auth0-python-4.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 cohere-5.14.0 coloredlogs-15.0.1 crewai-0.105.0 crewai_tools-0.37.0 dataclasses-json-0.6.7 deprecation-2.1.0 docker-7.1.0 durationpy-0.9 embedchain-0.1.127 fastapi-0.115.11 fastavro-1.10.0 gptcache-0.1.44 grpcio-1.71.0 grpcio-tools-1.71.0 httptools-0.6.4 httpx-0.27.2 httpx-sse-0.4.0 humanfriendly-10.0 importlib-metadata-8.5.0 instructor-1.7.3 jedi-0.19.2 json-repair-0.39.1 json5-0.10.0 jsonref-1.1.0 kubernetes-32.0.1 lancedb-0.21.0 langchain-cohere-0.3.5 langchain-experimental-0.3.4 langchain-openai-0.2.14 langchain_community-0.3.19 langsmith-0.1.147 litellm-1.60.2 marshmallow-3.26.1 mem0ai-0.1.67 mmh3-5.1.0 monotonic-1.6 mypy-extensions-1.0.0 nodeenv-1.9.1 onnxruntime-1.21.0 opentelemetry-api-1.30.0 opentelemetry-exporter-otlp-proto-common-1.30.0 opentelemetry-exporter-otlp-proto-grpc-1.30.0 opentelemetry-exporter-otlp-proto-http-1.30.0 opentelemetry-instrumentation-0.51b0 opentelemetry-instrumentation-asgi-0.51b0 opentelemetry-instrumentation-fastapi-0.51b0 opentelemetry-proto-1.30.0 opentelemetry-sdk-1.30.0 opentelemetry-semantic-conventions-0.51b0 opentelemetry-util-http-0.51b0 overrides-7.7.0 pdfminer.six-20231228 pdfplumber-0.11.5 portalocker-2.10.1 posthog-3.19.1 protobuf-5.29.3 psycopg2-binary-2.9.10 pydantic-settings-2.8.1 pypdf-5.3.1 pypdfium2-4.30.1 pypika-0.48.9 pyproject_hooks-1.2.0 pyright-1.1.396 pysbd-0.3.4 python-dotenv-1.0.1 pytube-15.0.0 pytz-2024.2 pyvis-0.3.2 qdrant-client-1.13.3 schema-0.7.7 starlette-0.46.1 tiktoken-0.7.0 tokenizers-0.20.3 tomli-2.2.1 tomli-w-1.2.0 types-requests-2.32.0.20250306 typing-inspect-0.9.0 uv-0.6.5 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "importlib_metadata"
                ]
              },
              "id": "5d7fe8e4bed94c2e9cceb4a70ed41137"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install crewai crewai_tools langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import LLM\n",
        "import os\n",
        "\n",
        "google_llm = LLM(\n",
        "              model='gemini/gemini-1.5-flash',\n",
        "              api_key='')"
      ],
      "metadata": {
        "id": "RcnL_WqtpxMB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Warning control\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "zm8Yszy9pxI3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent, Task, Crew"
      ],
      "metadata": {
        "id": "PdEnrirGpxFx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* By not setting allow_delegation=False, allow_delegation takes its default value of being True.\n",
        "* This means the agent can delegate its work to another agent which is better suited to do a particular task."
      ],
      "metadata": {
        "id": "rb1nHrgZ4HRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "support_agent = Agent(\n",
        "    llm=google_llm,\n",
        "    role=\"Senior Support Representative\",\n",
        "\tgoal=\"Be the most friendly and helpful \"\n",
        "        \"support representative in your team\",\n",
        "\tbackstory=(\n",
        "\t\t\"You work at crewAI (https://crewai.com) and \"\n",
        "        \" are now working on providing \"\n",
        "\t\t\"support to {customer}, a super important customer \"\n",
        "        \" for your company.\"\n",
        "\t\t\"You need to make sure that you provide the best support!\"\n",
        "\t\t\"Make sure to provide full complete answers, \"\n",
        "        \" and make no assumptions.\"\n",
        "\t),\n",
        "\tallow_delegation=False,\n",
        "\tverbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "s6XsEcTIpxC3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "support_quality_assurance_agent = Agent(\n",
        "  llm=google_llm,\n",
        "\trole=\"Support Quality Assurance Specialist\",\n",
        "\tgoal=\"Get recognition for providing the \"\n",
        "    \"best support quality assurance in your team\",\n",
        "\tbackstory=(\n",
        "\t\t\"You work at crewAI (https://crewai.com) and \"\n",
        "        \"are now working with your team \"\n",
        "\t\t\"on a request from {customer} ensuring that \"\n",
        "        \"the support representative is \"\n",
        "\t\t\"providing the best support possible.\\n\"\n",
        "\t\t\"You need to make sure that the support representative \"\n",
        "        \"is providing full\"\n",
        "\t\t\"complete answers, and make no assumptions.\"\n",
        "\t),\n",
        "\tverbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "h_dE4vZopxAK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Role Playing: Both agents have been given a role, goal and backstory.\n",
        "* Focus: Both agents have been prompted to get into the character of the roles they are playing.\n",
        "* Cooperation: Support Quality Assurance Agent can delegate work back to the Support Agent, allowing for these agents to work together."
      ],
      "metadata": {
        "id": "gMOq4MIG4SJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools"
      ],
      "metadata": {
        "id": "OWuCIM374Zbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai_tools import SerperDevTool, \\\n",
        "                         ScrapeWebsiteTool, \\\n",
        "                         WebsiteSearchTool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6uvQNMCpw9F",
        "outputId": "a57b0227-31f7-4ef5-a91e-ac59eb7b01d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_config.py:295: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/crewai_tools/tools/scrapegraph_scrape_tool/scrapegraph_scrape_tool.py:34: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  @validator(\"website_url\")\n",
            "/usr/local/lib/python3.11/dist-packages/crewai_tools/tools/selenium_scraping_tool/selenium_scraping_tool.py:26: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  @validator(\"website_url\")\n",
            "/usr/local/lib/python3.11/dist-packages/crewai_tools/tools/vision_tool/vision_tool.py:15: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  @validator(\"image_path_url\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs_scrape_tool = ScrapeWebsiteTool(\n",
        "    website_url=\"https://docs.crewai.com/how-to/Creating-a-Crew-and-kick-it-off/\"\n",
        ")"
      ],
      "metadata": {
        "id": "lM4rFX804cuz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different Ways to Give Agents Tools\n",
        "* Agent Level: The Agent can use the Tool(s) on any Task it performs.\n",
        "* Task Level: The Agent will only use the Tool(s) when performing that specific Task.\n",
        "\n",
        "Note: Task Tools override the Agent Tools.\n",
        "\n",
        "Creating Tasks\n",
        "* You are passing the Tool on the Task Level."
      ],
      "metadata": {
        "id": "60rtjOr24iGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "support_agent = Agent(\n",
        "    llm=google_llm,\n",
        "    role=\"Senior Support Representative\",\n",
        "\tgoal=\"Be the most friendly and helpful \"\n",
        "        \"support representative in your team\",\n",
        "\tbackstory=(\n",
        "\t\t\"You work at crewAI (https://crewai.com) and \"\n",
        "        \" are now working on providing \"\n",
        "\t\t\"support to {customer}, a super important customer \"\n",
        "        \" for your company.\"\n",
        "\t\t\"You need to make sure that you provide the best support!\"\n",
        "\t\t\"Make sure to provide full complete answers, \"\n",
        "        \" and make no assumptions.\"\n",
        "\t),\n",
        "\tallow_delegation=False,\n",
        "\tverbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "-j1QriEq4crO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "support_quality_assurance_agent = Agent(\n",
        "    llm=google_llm,\n",
        "    role=\"Support Quality Assurance Specialist\",\n",
        "\tgoal=\"Get recognition for providing the \"\n",
        "    \"best support quality assurance in your team\",\n",
        "\tbackstory=(\n",
        "\t\t\"You work at crewAI (https://crewai.com) and \"\n",
        "        \"are now working with your team \"\n",
        "\t\t\"on a request from {customer} ensuring that \"\n",
        "        \"the support representative is \"\n",
        "\t\t\"providing the best support possible.\\n\"\n",
        "\t\t\"You need to make sure that the support representative \"\n",
        "        \"is providing full\"\n",
        "\t\t\"complete answers, and make no assumptions.\"\n",
        "\t),\n",
        "\tverbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "Fdt54Mdq4co2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai_tools import SerperDevTool, \\\n",
        "                         ScrapeWebsiteTool, \\\n",
        "                         WebsiteSearchTool"
      ],
      "metadata": {
        "id": "-mCXj_FK4clz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_scrape_tool = ScrapeWebsiteTool(\n",
        "    website_url=\"https://docs.crewai.com/how-to/Creating-a-Crew-and-kick-it-off/\"\n",
        ")"
      ],
      "metadata": {
        "id": "Ub5O81EN6FDM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inquiry_resolution = Task(\n",
        "    description=(\n",
        "        \"{customer} just reached out with a super important ask:\\n\"\n",
        "\t    \"{inquiry}\\n\\n\"\n",
        "        \"{person} from {customer} is the one that reached out. \"\n",
        "\t\t\"Make sure to use everything you know \"\n",
        "        \"to provide the best support possible.\"\n",
        "\t\t\"You must strive to provide a complete \"\n",
        "        \"and accurate response to the customer's inquiry.\"\n",
        "    ),\n",
        "    expected_output=(\n",
        "\t    \"A detailed, informative response to the \"\n",
        "        \"customer's inquiry that addresses \"\n",
        "        \"all aspects of their question.\\n\"\n",
        "        \"The response should include references \"\n",
        "        \"to everything you used to find the answer, \"\n",
        "        \"including external data or solutions. \"\n",
        "        \"Ensure the answer is complete, \"\n",
        "\t\t\"leaving no questions unanswered, and maintain a helpful and friendly \"\n",
        "\t\t\"tone throughout.\"\n",
        "    ),\n",
        "\ttools=[docs_scrape_tool],\n",
        "    agent=support_agent,\n",
        ")"
      ],
      "metadata": {
        "id": "LdQEKBPN6E__"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quality_assurance_review = Task(\n",
        "    description=(\n",
        "        \"Review the response drafted by the Senior Support Representative for {customer}'s inquiry. \"\n",
        "        \"Ensure that the answer is comprehensive, accurate, and adheres to the \"\n",
        "\t\t\"high-quality standards expected for customer support.\\n\"\n",
        "        \"Verify that all parts of the customer's inquiry \"\n",
        "        \"have been addressed \"\n",
        "\t\t\"thoroughly, with a helpful and friendly tone.\\n\"\n",
        "        \"Check for references and sources used to \"\n",
        "        \" find the information, \"\n",
        "\t\t\"ensuring the response is well-supported and \"\n",
        "        \"leaves no questions unanswered.\"\n",
        "    ),\n",
        "    expected_output=(\n",
        "        \"A final, detailed, and informative response \"\n",
        "        \"ready to be sent to the customer.\\n\"\n",
        "        \"This response should fully address the \"\n",
        "        \"customer's inquiry, incorporating all \"\n",
        "\t\t\"relevant feedback and improvements.\\n\"\n",
        "\t\t\"Don't be too formal, we are a chill and cool company \"\n",
        "\t    \"but maintain a professional and friendly tone throughout.\"\n",
        "    ),\n",
        "    agent=support_quality_assurance_agent,\n",
        ")"
      ],
      "metadata": {
        "id": "CfWR48fx6E9x"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "crew = Crew(\n",
        "    agents=[support_agent, support_quality_assurance_agent],\n",
        "    tasks=[inquiry_resolution, quality_assurance_review],\n",
        "    verbose=True\n",
        "  )"
      ],
      "metadata": {
        "id": "ly8KBIO36E7Y"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\n",
        "    \"customer\": \"DeepLearningAI\",\n",
        "    \"person\": \"Andrew Ng\",\n",
        "    \"inquiry\": \"I need help with setting up a Crew \"\n",
        "               \"and kicking it off, specifically \"\n",
        "               \"how can I add memory to my crew? \"\n",
        "               \"Can you provide guidance?\"\n",
        "}\n",
        "\n",
        "result = crew.kickoff(inputs=inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tp32ngwC6E47",
        "outputId": "7e733672-9b45-493a-d7d2-1f6f25b1ac9a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[94m \n",
            "[2025-03-11 02:00:09][🚀 CREW 'CREW' STARTED, 11244F35-E330-443D-98D9-8E80926D8FEB]: 2025-03-11 02:00:09.373466\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-11 02:00:09][📋 TASK STARTED: DEEPLEARNINGAI JUST REACHED OUT WITH A SUPER IMPORTANT ASK:\n",
            "I NEED HELP WITH SETTING UP A CREW AND KICKING IT OFF, SPECIFICALLY HOW CAN I ADD MEMORY TO MY CREW? CAN YOU PROVIDE GUIDANCE?\n",
            "\n",
            "ANDREW NG FROM DEEPLEARNINGAI IS THE ONE THAT REACHED OUT. MAKE SURE TO USE EVERYTHING YOU KNOW TO PROVIDE THE BEST SUPPORT POSSIBLE.YOU MUST STRIVE TO PROVIDE A COMPLETE AND ACCURATE RESPONSE TO THE CUSTOMER'S INQUIRY.]: 2025-03-11 02:00:09.400170\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-11 02:00:09][🤖 AGENT 'SENIOR SUPPORT REPRESENTATIVE' STARTED TASK]: 2025-03-11 02:00:09.402248\u001b[00m\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Support Representative\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mDeepLearningAI just reached out with a super important ask:\n",
            "I need help with setting up a Crew and kicking it off, specifically how can I add memory to my crew? Can you provide guidance?\n",
            "\n",
            "Andrew Ng from DeepLearningAI is the one that reached out. Make sure to use everything you know to provide the best support possible.You must strive to provide a complete and accurate response to the customer's inquiry.\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-11 02:00:09][🤖 LLM CALL STARTED]: 2025-03-11 02:00:09.402627\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-11 02:00:12][✅ LLM CALL COMPLETED]: 2025-03-11 02:00:12.570310\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-11 02:00:12][🤖 TOOL USAGE STARTED: 'READ WEBSITE CONTENT']: 2025-03-11 02:00:12.572320\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-11 02:00:14][✅ TOOL USAGE FINISHED: 'READ WEBSITE CONTENT']: 2025-03-11 02:00:14.847696\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Support Representative\u001b[00m\n",
            "\u001b[95m## Using tool:\u001b[00m \u001b[92mRead website content\u001b[00m\n",
            "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
            "\"{}\"\u001b[00m\n",
            "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
            "Introduction - CrewAI CrewAI home page Search CrewAI docs crewAIInc / crewAI crewAIInc / crewAI Search... Navigation Get Started Introduction Get Started Examples CrewAI home page Community Changelog Get Started Introduction Installation Quickstart Guides Concepts Agents Crews Flows Core Concepts Agents Tasks Crews Flows Knowledge LLMs Processes Collaboration Training Memory Planning Testing CLI Tools Using LangChain Tools Using LlamaIndex Tools How to Guides Create Custom Tools Sequential Processes Hierarchical Process Create Your Own Manager Agent Connect to any LLM Customize Agents Using Multimodal Agents Coding Agents Force Tool Output as Result Human Input on Execution Kickoff Crew Asynchronously Kickoff Crew for Each Replay Tasks from Latest Crew Kickoff Conditional Tasks Agent Monitoring with AgentOps Agent Monitoring with Langtrace Agent Monitoring with MLflow Agent Monitoring with OpenLIT Agent Monitoring with Portkey Agent Monitoring with Langfuse Tools AI Mind Tool Brave Search Browserbase Web Loader Code Docs RAG Search Code Interpreter Composio Tool CSV RAG Search DALL-E Tool Directory RAG Search Directory Read DOCX RAG Search EXA Search Web Loader File Read File Write Firecrawl Crawl Website Firecrawl Scrape Website Firecrawl Search Github Search Hyperbrowser Load Tool Linkup Search Tool LlamaIndex Tool Google Serper Search S3 Reader Tool S3 Writer Tool Scrapegraph Scrape Tool Scrape Element From Website Tool JSON RAG Search MDX RAG Search MySQL RAG Search MultiOn Tool NL2SQL Tool Patronus Evaluation Tools PDF RAG Search PG RAG Search Qdrant Vector Search Tool RAG Tool Scrape Website Scrapfly Scrape Website Tool Selenium Scraper Snowflake Search Tool Spider Scraper TXT RAG Search Vision Tool Weaviate Vector Search Website RAG Search XML RAG Search YouTube Channel RAG Search YouTube Video RAG Search Telemetry Telemetry Get Started Introduction Build AI agent teams that work together to tackle complex tasks ‚Äã What is CrewAI?\n",
            "CrewAI is a lean, lightning-fast Python framework built entirely from scratch‚Äîcompletely independent of LangChain or other agent frameworks.\n",
            "CrewAI empowers developers with both high-level simplicity and precise low-level control, ideal for creating autonomous AI agents tailored to any scenario:\n",
            "CrewAI Crews : Optimize for autonomy and collaborative intelligence, enabling you to create AI teams where each agent has specific roles, tools, and goals.\n",
            "CrewAI Flows : Enable granular, event-driven control, single LLM calls for precise task orchestration and supports Crews natively.\n",
            "With over 100,000 developers certified through our community courses, CrewAI is rapidly becoming the standard for enterprise-ready AI automation.\n",
            "‚Äã How Crews Work\n",
            "Just like a company has departments (Sales, Engineering, Marketing) working together under leadership to achieve business goals, CrewAI helps you create an organization of AI agents with specialized roles collaborating to accomplish complex tasks.\n",
            "CrewAI Framework Overview\n",
            "Component Description Key Features Crew The top-level organization ‚Ä¢ Manages AI agent teams ‚Ä¢ Oversees workflows ‚Ä¢ Ensures collaboration ‚Ä¢ Delivers outcomes AI Agents Specialized team members ‚Ä¢ Have specific roles (researcher, writer) ‚Ä¢ Use designated tools ‚Ä¢ Can delegate tasks ‚Ä¢ Make autonomous decisions Process Workflow management system ‚Ä¢ Defines collaboration patterns ‚Ä¢ Controls task assignments ‚Ä¢ Manages interactions ‚Ä¢ Ensures efficient execution Tasks Individual assignments ‚Ä¢ Have clear objectives ‚Ä¢ Use specific tools ‚Ä¢ Feed into larger process ‚Ä¢ Produce actionable results\n",
            "‚Äã How It All Works Together\n",
            "The Crew organizes the overall operation\n",
            "AI Agents work on their specialized tasks\n",
            "The Process ensures smooth collaboration\n",
            "Tasks get completed to achieve the goal\n",
            "‚Äã Key Features\n",
            "Role-Based Agents Create specialized agents with defined roles, expertise, and goals - from researchers to analysts to writers Flexible Tools Equip agents with custom tools and APIs to interact with external services and data sources Intelligent Collaboration Agents work together, sharing insights and coordinating tasks to achieve complex objectives Task Management Define sequential or parallel workflows, with agents automatically handling task dependencies\n",
            "‚Äã How Flows Work\n",
            "While Crews excel at autonomous collaboration, Flows provide structured automations, offering granular control over workflow execution. Flows ensure tasks are executed reliably, securely, and efficiently, handling conditional logic, loops, and dynamic state management with precision. Flows integrate seamlessly with Crews, enabling you to balance high autonomy with exacting control.\n",
            "CrewAI Framework Overview\n",
            "Component Description Key Features Flow Structured workflow orchestration ‚Ä¢ Manages execution paths ‚Ä¢ Handles state transitions ‚Ä¢ Controls task sequencing ‚Ä¢ Ensures reliable execution Events Triggers for workflow actions ‚Ä¢ Initiate specific processes ‚Ä¢ Enable dynamic responses ‚Ä¢ Support conditional branching ‚Ä¢ Allow for real-time adaptation States Workflow execution contexts ‚Ä¢ Maintain execution data ‚Ä¢ Enable persistence ‚Ä¢ Support resumability ‚Ä¢ Ensure execution integrity Crew Support Enhances workflow automation ‚Ä¢ Injects pockets of agency when needed ‚Ä¢ Complements structured workflows ‚Ä¢ Balances automation with intelligence ‚Ä¢ Enables adaptive decision-making\n",
            "‚Äã Key Capabilities\n",
            "Event-Driven Orchestration Define precise execution paths responding dynamically to events Fine-Grained Control Manage workflow states and conditional execution securely and efficiently Native Crew Integration Effortlessly combine with Crews for enhanced autonomy and intelligence Deterministic Execution Ensure predictable outcomes with explicit control flow and error handling\n",
            "‚Äã When to Use Crews vs. Flows\n",
            "Understanding when to use Crews versus Flows is key to maximizing the potential of CrewAI in your applications.\n",
            "Use Case Recommended Approach Why? Open-ended research Crews When tasks require creative thinking, exploration, and adaptation Content generation Crews For collaborative creation of articles, reports, or marketing materials Decision workflows Flows When you need predictable, auditable decision paths with precise control API orchestration Flows For reliable integration with multiple external services in a specific sequence Hybrid applications Combined approach Use Flows to orchestrate overall process with Crews handling complex subtasks\n",
            "‚Äã Decision Framework\n",
            "Choose Crews when: You need autonomous problem-solving, creative collaboration, or exploratory tasks\n",
            "Choose Flows when: You require deterministic outcomes, auditability, or precise control over execution\n",
            "Combine both when: Your application needs both structured processes and pockets of autonomous intelligence\n",
            "‚Äã Why Choose CrewAI?\n",
            "üß† Autonomous Operation : Agents make intelligent decisions based on their roles and available tools\n",
            "üìù Natural Interaction : Agents communicate and collaborate like human team members\n",
            "üõ†Ô∏è Extensible Design : Easy to add new tools, roles, and capabilities\n",
            "üöÄ Production Ready : Built for reliability and scalability in real-world applications\n",
            "üîí Security-Focused : Designed with enterprise security requirements in mind\n",
            "üí∞ Cost-Efficient : Optimized to minimize token usage and API calls\n",
            "‚Äã Ready to Start Building?\n",
            "Build Your First Crew Step-by-step tutorial to create a collaborative AI team that works together to solve complex problems. Build Your First Flow Learn how to create structured, event-driven workflows with precise control over execution.\n",
            "Install CrewAI Get started with CrewAI in your development environment. Quick Start Follow our quickstart guide to create your first CrewAI agent and get hands-on experience. Join the Community Connect with other developers, get help, and share your CrewAI experiences. Was this page helpful? Yes No Installation website x github linkedin youtube Powered by Mintlify On this page What is CrewAI? How Crews Work How It All Works Together Key Features How Flows Work Key Capabilities When to Use Crews vs. Flows Decision Framework Why Choose CrewAI? Ready to Start Building?\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-11 02:00:14][🤖 LLM CALL STARTED]: 2025-03-11 02:00:14.848340\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-11 02:00:17][✅ LLM CALL COMPLETED]: 2025-03-11 02:00:17.752814\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSenior Support Representative\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "Thank you for reaching out, Andrew! I'm happy to help you with setting up your Crew and kicking it off.  To best assist you with adding memory to your Crew, could you please clarify what you mean by \"adding memory\"?  Are you looking to:\n",
            "\n",
            "*   **Store information between runs of the Crew?**  If so, we would need to explore options for persistent storage, such as databases or filesystems, to handle this.\n",
            "*   **Increase the working memory of individual agents within the Crew?**  This might involve adjusting parameters within your agents' code, potentially affecting the amount of information they can process at once.\n",
            "*   **Implement a memory mechanism for the Crew as a whole?**  This would likely involve designing a shared memory system accessible to all agents within the Crew.  We could discuss appropriate data structures and methods for efficient inter-agent communication.\n",
            "\n",
            "\n",
            "To provide you with the best solution, please describe the specific task your Crew is performing and how you envision memory being used to improve its functionality.  The more detail you can provide, the better I can assist you.  We can then explore the appropriate approach together.\n",
            "\n",
            "I found the CrewAI documentation very helpful in understanding the overall framework, but it didn't directly address memory management in the context of Crews.  However, I'm confident that with more information from you, we can find the perfect solution.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-11 02:00:17][✅ AGENT 'SENIOR SUPPORT REPRESENTATIVE' COMPLETED TASK]: 2025-03-11 02:00:17.754959\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-11 02:00:17][✅ TASK COMPLETED: DEEPLEARNINGAI JUST REACHED OUT WITH A SUPER IMPORTANT ASK:\n",
            "I NEED HELP WITH SETTING UP A CREW AND KICKING IT OFF, SPECIFICALLY HOW CAN I ADD MEMORY TO MY CREW? CAN YOU PROVIDE GUIDANCE?\n",
            "\n",
            "ANDREW NG FROM DEEPLEARNINGAI IS THE ONE THAT REACHED OUT. MAKE SURE TO USE EVERYTHING YOU KNOW TO PROVIDE THE BEST SUPPORT POSSIBLE.YOU MUST STRIVE TO PROVIDE A COMPLETE AND ACCURATE RESPONSE TO THE CUSTOMER'S INQUIRY.]: 2025-03-11 02:00:17.755383\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-11 02:00:17][📋 TASK STARTED: REVIEW THE RESPONSE DRAFTED BY THE SENIOR SUPPORT REPRESENTATIVE FOR DEEPLEARNINGAI'S INQUIRY. ENSURE THAT THE ANSWER IS COMPREHENSIVE, ACCURATE, AND ADHERES TO THE HIGH-QUALITY STANDARDS EXPECTED FOR CUSTOMER SUPPORT.\n",
            "VERIFY THAT ALL PARTS OF THE CUSTOMER'S INQUIRY HAVE BEEN ADDRESSED THOROUGHLY, WITH A HELPFUL AND FRIENDLY TONE.\n",
            "CHECK FOR REFERENCES AND SOURCES USED TO  FIND THE INFORMATION, ENSURING THE RESPONSE IS WELL-SUPPORTED AND LEAVES NO QUESTIONS UNANSWERED.]: 2025-03-11 02:00:17.767076\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-11 02:00:17][🤖 AGENT 'SUPPORT QUALITY ASSURANCE SPECIALIST' STARTED TASK]: 2025-03-11 02:00:17.769375\u001b[00m\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSupport Quality Assurance Specialist\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mReview the response drafted by the Senior Support Representative for DeepLearningAI's inquiry. Ensure that the answer is comprehensive, accurate, and adheres to the high-quality standards expected for customer support.\n",
            "Verify that all parts of the customer's inquiry have been addressed thoroughly, with a helpful and friendly tone.\n",
            "Check for references and sources used to  find the information, ensuring the response is well-supported and leaves no questions unanswered.\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-11 02:00:17][🤖 LLM CALL STARTED]: 2025-03-11 02:00:17.769683\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-11 02:00:24][✅ LLM CALL COMPLETED]: 2025-03-11 02:00:24.472282\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mSupport Quality Assurance Specialist\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "Subject: Re: Setting up your Crew and Adding Memory\n",
            "\n",
            "Hi Andrew,\n",
            "\n",
            "Thanks for reaching out!  We're excited to help you get your Crew up and running smoothly.  I understand you're looking to add memory to your Crew, and to give you the best possible assistance, let's clarify what you mean by \"adding memory\" in the context of your Crew's operation.\n",
            "\n",
            "Your question is a great one, and it highlights an important aspect of CrewAI that isn't explicitly covered in all of our documentation.  While our docs explain Crew creation and agent interaction, the nuances of persistent storage and internal memory management require a more tailored approach.\n",
            "\n",
            "To help me tailor the perfect solution for you, could you please provide some more details?  Let's break down the possibilities:\n",
            "\n",
            "\n",
            "**1. Persistent Storage (Remembering between Runs):**\n",
            "\n",
            "*   **Scenario:** Your Crew needs to remember information from one run to the next.  For example, perhaps your Crew is analyzing data streams, and it needs to maintain a running total or a historical record of events.\n",
            "\n",
            "*   **Solution:**  This requires persistent storage outside of the Crew's immediate memory.  We can integrate with various solutions depending on your needs and preferences.  This could include:\n",
            "    *   **Databases:**  SQL databases (like PostgreSQL or MySQL) or NoSQL databases (like MongoDB or Cassandra) provide robust and scalable solutions for storing and retrieving information.  We can help you choose the best database based on your data structure and access patterns.\n",
            "    *   **Filesystems:**  For simpler scenarios, storing data in files (e.g., JSON, CSV, or custom formats) could suffice. We can help you design the file structure for optimal data access and management.\n",
            "    *   **Cloud Storage:** Services like AWS S3 or Google Cloud Storage offer scalable and cost-effective storage options, especially for large datasets.\n",
            "\n",
            "*   **Implementation Considerations:** We'll need to discuss your data structure, expected data volume, access frequency, and any specific requirements for data integrity and security.\n",
            "\n",
            "\n",
            "**2. Increasing Agent Working Memory:**\n",
            "\n",
            "*   **Scenario:** Individual agents within your Crew are running out of memory to process their assigned tasks.  This might manifest as performance issues, crashes, or incorrect results.\n",
            "\n",
            "*   **Solution:**  This requires adjusting the parameters within your agent's code, depending on the programming language and framework you're using.  Increasing the available memory might involve:\n",
            "    *   **Python:**  Optimizing data structures and using memory-efficient libraries.  We can help you identify bottlenecks and optimize memory usage in your code.\n",
            "    *   **Other Languages:**  The approach will depend on the language, but it generally involves similar principles of optimizing data structures and algorithms.\n",
            "\n",
            "*   **Implementation Considerations:** To help me advise you effectively, please provide details about the programming languages and frameworks used, the agent's code, and the nature of the data it processes.  Error messages or performance metrics will also be incredibly helpful.\n",
            "\n",
            "\n",
            "**3. Shared Memory for Inter-Agent Communication:**\n",
            "\n",
            "*   **Scenario:** Your agents need to share information and coordinate their actions efficiently.  Passing large amounts of data between agents repeatedly can impact performance.\n",
            "\n",
            "*   **Solution:**  Implementing a shared memory system (using appropriate data structures and locking mechanisms) allows agents to access and modify information in a controlled manner.  This could involve:\n",
            "    *   **Queues:** Message queues (e.g., RabbitMQ, Kafka) enable asynchronous communication between agents, reducing contention and improving performance.\n",
            "    *   **Shared Memory Data Structures:**  In languages like Python, using `multiprocessing.shared_memory` or similar mechanisms can enable efficient shared memory access.\n",
            "\n",
            "*   **Implementation Considerations:**  We need to understand the communication pattern between your agents, the type and size of data being shared, and the level of concurrency involved.  This will determine the most suitable approach for shared memory management.\n",
            "\n",
            "\n",
            "\n",
            "To give you the most effective solution, please tell me more about:\n",
            "\n",
            "*   The specific task your Crew is designed to perform.\n",
            "*   The architecture of your Crew (number of agents, their roles, and interactions).\n",
            "*   The programming language(s) and frameworks used in your agent code.\n",
            "*   The size and type of data your Crew is processing.\n",
            "*   Any specific performance issues or limitations you are experiencing.\n",
            "\n",
            "\n",
            "Once we have a clear picture of your requirements, we can collaborate on the best approach for adding the necessary \"memory\" to your Crew. We look forward to helping you achieve your goals!\n",
            "\n",
            "Best regards,\n",
            "\n",
            "The CrewAI Support Team\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-11 02:00:24][✅ AGENT 'SUPPORT QUALITY ASSURANCE SPECIALIST' COMPLETED TASK]: 2025-03-11 02:00:24.474023\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-11 02:00:24][✅ TASK COMPLETED: REVIEW THE RESPONSE DRAFTED BY THE SENIOR SUPPORT REPRESENTATIVE FOR DEEPLEARNINGAI'S INQUIRY. ENSURE THAT THE ANSWER IS COMPREHENSIVE, ACCURATE, AND ADHERES TO THE HIGH-QUALITY STANDARDS EXPECTED FOR CUSTOMER SUPPORT.\n",
            "VERIFY THAT ALL PARTS OF THE CUSTOMER'S INQUIRY HAVE BEEN ADDRESSED THOROUGHLY, WITH A HELPFUL AND FRIENDLY TONE.\n",
            "CHECK FOR REFERENCES AND SOURCES USED TO  FIND THE INFORMATION, ENSURING THE RESPONSE IS WELL-SUPPORTED AND LEAVES NO QUESTIONS UNANSWERED.]: 2025-03-11 02:00:24.474260\u001b[00m\n",
            "\u001b[1m\u001b[94m \n",
            "[2025-03-11 02:00:24][✅ CREW 'CREW' COMPLETED, 11244F35-E330-443D-98D9-8E80926D8FEB]: 2025-03-11 02:00:24.488318\u001b[00m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "Markdown(str(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TRKylQ1m6E2n",
        "outputId": "4a63f467-57c1-4921-f6cc-316b06e0d1da"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Subject: Re: Setting up your Crew and Adding Memory\n\nHi Andrew,\n\nThanks for reaching out!  We're excited to help you get your Crew up and running smoothly.  I understand you're looking to add memory to your Crew, and to give you the best possible assistance, let's clarify what you mean by \"adding memory\" in the context of your Crew's operation.\n\nYour question is a great one, and it highlights an important aspect of CrewAI that isn't explicitly covered in all of our documentation.  While our docs explain Crew creation and agent interaction, the nuances of persistent storage and internal memory management require a more tailored approach.\n\nTo help me tailor the perfect solution for you, could you please provide some more details?  Let's break down the possibilities:\n\n\n**1. Persistent Storage (Remembering between Runs):**\n\n*   **Scenario:** Your Crew needs to remember information from one run to the next.  For example, perhaps your Crew is analyzing data streams, and it needs to maintain a running total or a historical record of events.\n\n*   **Solution:**  This requires persistent storage outside of the Crew's immediate memory.  We can integrate with various solutions depending on your needs and preferences.  This could include:\n    *   **Databases:**  SQL databases (like PostgreSQL or MySQL) or NoSQL databases (like MongoDB or Cassandra) provide robust and scalable solutions for storing and retrieving information.  We can help you choose the best database based on your data structure and access patterns.\n    *   **Filesystems:**  For simpler scenarios, storing data in files (e.g., JSON, CSV, or custom formats) could suffice. We can help you design the file structure for optimal data access and management.\n    *   **Cloud Storage:** Services like AWS S3 or Google Cloud Storage offer scalable and cost-effective storage options, especially for large datasets.\n\n*   **Implementation Considerations:** We'll need to discuss your data structure, expected data volume, access frequency, and any specific requirements for data integrity and security.\n\n\n**2. Increasing Agent Working Memory:**\n\n*   **Scenario:** Individual agents within your Crew are running out of memory to process their assigned tasks.  This might manifest as performance issues, crashes, or incorrect results.\n\n*   **Solution:**  This requires adjusting the parameters within your agent's code, depending on the programming language and framework you're using.  Increasing the available memory might involve:\n    *   **Python:**  Optimizing data structures and using memory-efficient libraries.  We can help you identify bottlenecks and optimize memory usage in your code.\n    *   **Other Languages:**  The approach will depend on the language, but it generally involves similar principles of optimizing data structures and algorithms.\n\n*   **Implementation Considerations:** To help me advise you effectively, please provide details about the programming languages and frameworks used, the agent's code, and the nature of the data it processes.  Error messages or performance metrics will also be incredibly helpful.\n\n\n**3. Shared Memory for Inter-Agent Communication:**\n\n*   **Scenario:** Your agents need to share information and coordinate their actions efficiently.  Passing large amounts of data between agents repeatedly can impact performance.\n\n*   **Solution:**  Implementing a shared memory system (using appropriate data structures and locking mechanisms) allows agents to access and modify information in a controlled manner.  This could involve:\n    *   **Queues:** Message queues (e.g., RabbitMQ, Kafka) enable asynchronous communication between agents, reducing contention and improving performance.\n    *   **Shared Memory Data Structures:**  In languages like Python, using `multiprocessing.shared_memory` or similar mechanisms can enable efficient shared memory access.\n\n*   **Implementation Considerations:**  We need to understand the communication pattern between your agents, the type and size of data being shared, and the level of concurrency involved.  This will determine the most suitable approach for shared memory management.\n\n\n\nTo give you the most effective solution, please tell me more about:\n\n*   The specific task your Crew is designed to perform.\n*   The architecture of your Crew (number of agents, their roles, and interactions).\n*   The programming language(s) and frameworks used in your agent code.\n*   The size and type of data your Crew is processing.\n*   Any specific performance issues or limitations you are experiencing.\n\n\nOnce we have a clear picture of your requirements, we can collaborate on the best approach for adding the necessary \"memory\" to your Crew. We look forward to helping you achieve your goals!\n\nBest regards,\n\nThe CrewAI Support Team"
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8KIHo7ah6E0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j7lpnGDA6Ew8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}